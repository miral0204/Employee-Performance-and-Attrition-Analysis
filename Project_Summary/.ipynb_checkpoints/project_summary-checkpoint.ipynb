{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc233cc-58e1-4bc1-aed2-5e7598f06644",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Project Summary**\n",
    "\n",
    "---\n",
    "\n",
    "# **Employee Performance Analysis**  \n",
    "#### **INX Future Inc.** | Code: 10281  \n",
    "#### **Miral Katpara** | CDS Datamites \n",
    "\n",
    "---\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "*INX Future Inc*, a leading provider of data analytics and automation solutions, has faced recent challenges with declining employee performance, increasing service delivery escalations, and a notable drop in client satisfaction by 8 percentage points. My task is to analyze the current employee data and uncover the key factors driving these performance issues and attrition rates.\n",
    "\n",
    "---\n",
    "\n",
    "### **Project Scope and Objectives**\n",
    "\n",
    "- My solution aims to provide *INX Future Inc* with a comprehensive analysis of employee performance data, identifying the underlying causes of performance decline and the factors affecting employee satisfaction.\n",
    "- We will develop predictive models to pinpoint potential indicators of non-performing employees and deliver actionable insights. These insights will guide INX's management in implementing targeted interventions to enhance employee performance while maintaining high morale and sustaining the company's reputation as a top employer.\n",
    "\n",
    "---\n",
    "\n",
    "## **Project Outline**\n",
    "\n",
    "In this project, we addressed two key problems: predicting employee attrition and predicting employee performance ratings. We employed several machine learning models to analyze and forecast these aspects, with a focus on understanding employee behavior, improving workforce management, and enhancing company performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **Data Preprocessing and EDA**\n",
    "\n",
    "### **Exploratory Data Analysis (EDA)**\n",
    "\n",
    "From the EDA, we derived several critical insights:\n",
    "\n",
    "- **Gender Distribution**: The dataset shows a 60/40 split between male and female employees.\n",
    "- **Education Backgrounds**: Employees come from six different education backgrounds, with Life Sciences and Marketing being predominant.\n",
    "- **Job Roles**: Research and Development (R&D) roles and Sales positions are crucial for the company's performance.\n",
    "- **Job Satisfaction**: Most employees reported high job satisfaction, good work environment, and work-life balance. However, a small segment with poor ratings indicates a need for improvement.\n",
    "- **Performance Ratings**: While most employees perform well, only 11% achieved an outstanding performance rating. This is an area for potential improvement.\n",
    "- **Attrition Rates**: Attrition rates are relatively low. Key factors influencing attrition include job satisfaction, work environment, and education background. Notably, Life Science employees and Developers/Sales Executives showed higher attrition rates.\n",
    "\n",
    "---\n",
    "\n",
    "### **Data Cleaning and Processing**\n",
    "\n",
    "- **Handling Categorical Data**: \n",
    "  - Categorical features were converted into numerical format for model compatibility.\n",
    "  - **Label Encoding** was used for categorical variables without inherent order (e.g., Gender, Department).\n",
    "  - **Ordinal Encoding** was applied to variables with a rank or hierarchy (e.g., Job Satisfaction, Job Level).\n",
    "\n",
    "- **Outlier Detection and Management**: Outliers were identified and handled to ensure they did not skew the model's performance. This involved either removing them or replacing them with mean/median values.\n",
    "\n",
    "- **Scaling**: Features were standardized using `StandardScaler` to ensure uniformity in the training process, which helps improve model performance.\n",
    "\n",
    "- **SMOTE for Class Imbalance**:\n",
    "  - **SMOTE (Synthetic Minority Over-sampling Technique)** was used to address class imbalance in the attrition dataset. By generating synthetic samples for the minority class, SMOTE helps the model learn better and improve performance.\n",
    "\n",
    "### Encoding categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "### Applying StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "### SMOTE for class imbalance\n",
    "smote = SMOTE()\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab331197-4960-4066-a74e-2428359c0e72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Model Training and Evaluation**\n",
    "\n",
    "### **Attrition Prediction**\n",
    "\n",
    "We trained several models to predict employee attrition, evaluating each model's performance based on accuracy, interpretability, and robustness.\n",
    "\n",
    "#### **Logistic Regression:**\n",
    "- **Accuracy**: 0.88\n",
    "- **Explanation**: Logistic Regression is a linear model that estimates the probability of a binary outcome using a logistic function. It is simple, interpretable, and effective for binary classification tasks like predicting whether an employee will leave the company.\n",
    "- **Mathematical Notation**: The model estimates the probability \\( P(y=1 \\mid X) \\) using the equation:\n",
    "  $$\n",
    "  P(y=1 \\mid X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_n X_n)}}\n",
    "  $$\n",
    "  where \\( \\beta \\) represents the model coefficients.\n",
    "\n",
    "#### **Neural Network:**\n",
    "- **Accuracy**: 0.71\n",
    "- **Explanation**: Neural Networks, inspired by the human brain, consist of interconnected neurons organized in layers. They capture complex patterns and relationships in the data, making them suitable for high-dimensional data.\n",
    "- **Mathematical Notation**: The output of a neuron is given by:\n",
    "  $$\n",
    "  a = \\sigma(WX + b)\n",
    "  $$\n",
    "  where \\( \\sigma \\) is the activation function, \\( W \\) is the weight matrix, \\( X \\) is the input, and \\( b \\) is the bias term.\n",
    "\n",
    "#### **K-Nearest Neighbors (KNN):**\n",
    "- **Accuracy**: 0.85\n",
    "- **Explanation**: KNN is a simple, non-parametric algorithm that classifies a sample based on the majority class among its K-nearest neighbors. It's effective for datasets with non-linear decision boundaries.\n",
    "- **Mathematical Notation**: The distance metric (e.g., Euclidean distance) is used to determine the nearest neighbors:\n",
    "  $$\n",
    "  d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n",
    "  $$\n",
    "\n",
    "#### **Support Vector Classification (SVC):**\n",
    "- **Accuracy**: 0.87\n",
    "- **Explanation**: SVC is a powerful classifier that finds the hyperplane separating the classes with the maximum margin. It handles high-dimensional data well and is effective for binary classification.\n",
    "- **Mathematical Notation**: The optimization problem solved by SVC is:\n",
    "  $$\n",
    "  \\min_{w, b} \\frac{1}{2} ||w||^2 \\text{ subject to } y_i (w^T x_i + b) \\geq 1 \\text{ for all } i\n",
    "  $$\n",
    "\n",
    "#### **Random Forest:**\n",
    "- **Accuracy**: 0.94\n",
    "- **Explanation**: Random Forest is an ensemble method that builds multiple decision trees and merges them to get a more accurate and stable prediction. It's robust against overfitting and handles complex feature interactions effectively.\n",
    "- **Mathematical Notation**: The final prediction is made by aggregating the predictions of individual trees, typically using majority voting:\n",
    "  $$\n",
    "  \\hat{y} = \\text{mode}(\\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_n)\n",
    "  $$\n",
    "\n",
    "#### **XGBoost:**\n",
    "- **Accuracy**: 0.86\n",
    "- **Explanation**: XGBoost is an advanced boosting algorithm that iteratively improves weak learners (e.g., decision trees) by focusing on the errors made by previous models. It's known for its high performance and ability to manage missing data and outliers efficiently.\n",
    "- **Mathematical Notation**: The objective function is minimized using gradient descent:\n",
    "  $$\n",
    "  \\text{Obj}(\\theta) = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\Omega(f_k)\n",
    "  $$\n",
    "  where \\( \\Omega(f_k) \\) is a regularization term.\n",
    "\n",
    "---\n",
    "\n",
    "### **Performance Rating Prediction**\n",
    "\n",
    "We also trained models to predict employee performance ratings, aiming to identify high-performing employees and those in need of support.\n",
    "\n",
    "#### **LightGBM:**\n",
    "- **Accuracy**: 95%\n",
    "- **Explanation**: LightGBM is a gradient boosting framework that uses tree-based learning algorithms. It's designed for efficiency and scalability, making it suitable for large datasets with a large number of features.\n",
    "- **Mathematical Notation**: The model uses a histogram-based algorithm for splitting nodes, which reduces the computational cost:\n",
    "  $$\n",
    "  \\text{gain} = \\text{split\\_gain} - \\gamma \\times \\text{penalty}\n",
    "  $$\n",
    "\n",
    "#### **K-Nearest Neighbors (KNN):**\n",
    "- **Accuracy**: 78%\n",
    "- **Explanation**: As with attrition prediction, KNN classifies a sample based on its nearest neighbors. While simple, KNN may struggle with high-dimensional data, which can lead to lower accuracy in this context.\n",
    "\n",
    "#### **Random Forest:**\n",
    "- **Accuracy**: 82%\n",
    "- **Explanation**: Random Forest continues to perform well, offering robustness and reduced overfitting, making it a reliable choice for predicting performance ratings.\n",
    "\n",
    "#### **Neural Network:**\n",
    "- **Accuracy**: 86%\n",
    "- **Explanation**: Neural Networks capture the complex relationships between features and performance ratings, providing a nuanced prediction model. However, they require careful tuning to avoid overfitting.\n",
    "\n",
    "#### **XGBoost:**\n",
    "- **Accuracy**: 93%\n",
    "- **Explanation**: XGBoost's strong performance is consistent across tasks, providing efficient and accurate predictions while being robust to overfitting and capable of handling complex feature interactions.\n",
    "\n",
    "---\n",
    "\n",
    "## **Recommendations**\n",
    "\n",
    "- **For Attrition Prediction**: Random Forest and XGBoost provided the highest accuracy. Given Random Forest's robustness and interpretability, it is recommended for predicting attrition. However, for large-scale implementations where computational efficiency is critical, XGBoost may be preferred.\n",
    "\n",
    "- **For Performance Rating Prediction**: LightGBM and XGBoost both performed well, with LightGBM showing a slight edge in accuracy. LightGBM is recommended for its efficiency and scalability, particularly in large datasets. However, XGBoost remains a strong alternative, especially when dealing with more complex or noisy data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "* #### The analysis and models developed provide *INX Future Inc* with actionable insights into employee performance and attrition. These insights can guide the management in implementing targeted strategies to improve employee satisfaction, reduce attrition, and enhance overall company performance. \n",
    "* #### The predictive models, especially Random Forest and LightGBM, offer reliable tools for ongoing monitoring and intervention.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
